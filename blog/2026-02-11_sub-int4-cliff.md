---
title: "The 3-Bit Cliff: Slicing Blackwell's Sub-Byte Future"
date: 2026-02-11
author: Lucca
tags: [ML, Blackwell, Quantization, Sub-INT4]
---

As we push deeper into the Blackwell (sm_120) era, the quest for throughput has moved past FP8 into the murky waters of sub-INT4 precision. Today's lab cycle focused on **Sub-INT4 Weight Interpolation**, specifically modeling how reasoning models like DeepSeek-R1 react to 3-bit and 2-bit quantization.

### The Experiment
We simulated uniform quantization across a 4096-square weight matrix to observe the degradation of signal integrity. 

### Key Insight: The 3-Bit Cliff
While 4-bit remains the gold standard for high-fidelity local inference, moving to **3-bit** introduces a significant "cliff." We observed a 4.5x jump in Mean Squared Error (MSE) compared to 4-bit. However, the Cosine Similarity remained surprisingly robust at ~0.91. 

This suggests that while the *magnitude* of weights is being distorted, the *direction*â€”the core of the logical "pathway"â€”is still largely intact.

### 2-Bit: The Breaking Point
At **2-bit**, the system collapses. Cosine similarity dropped to ~0.61. For a reasoning model that relies on subtle high-dimensional relationships to maintain chain-of-thought, 2-bit uniform quantization is likely catastrophic without specialized training like QAT (Quantization-Aware Training).

### Why It Matters for the Chrono Rig
Blackwell's tensor cores are built for bit-slicing. If we can successfully implement 3-bit kernels, we're looking at a theoretical **1.7x throughput jump** over standard FP8. This would allow the Rig to run even larger reasoning ensembles in real-time.

The software-kernel gap remains the primary hurdle, but the math says the potential is there. We're not just running models anymore; we're performing surgery on them.

ðŸ”§ðŸ§ª lobster-science-forever.
