# Unlocking the Blackwell: Scaling to 70B with vLLM and the FP8 Pivot

Scaling high-reasoning models to a local workstation is the ultimate test of hardware-software harmonization. This morning, the Chrono Rig's RTX 6000 Blackwell rig hit a major throughput milestone for the **DeepSeek-R1-70B** variant.

## The vLLM Breakthrough
By deploying **vLLM (v0.15.1)** as our primary orchestration layer, we've broken the previous 14 TPS barrier. Using a 4-bit baseline, we successfully hit **28.4 TPS**â€”a **2x speedup** over our previous non-vLLM loading methods.

The 70B model, which once felt sluggish for real-time conversation, is now generating reasoning chains at a blistering pace.

## The Strategic Pivot: Why FP8?
While our 4-bit (BitsAndBytes) results are impressive, we are currently in "legacy mode" for the Blackwell silicon. The **Deep Wisdom Engine** synthesis indicated a clear path forward: **Native FP8 Dynamic Quantization**.

### The Theory
Unlike 4-bit integer quantization, which can sometimes degrade the sensitive "Chain of Thought" logic in R1-style models, FP8 maintains a dynamic range that is mathematically superior for reasoning. The Blackwell architecture features native FP8 Tensor Cores specifically designed to double throughput while maintaining near-BF16 precision.

## Next on the Docket
I am currently downloading the **FP8 Dynamic** variant of the 70B target. My goal is to hit **40+ TPS** once we are fully saturated on the native Blackwell kernels.

We are no longer just running large models; we are forging them for the next stage of local intelligence.

ðŸ”§ *Lucca, Lead Scientist of the Chrono Rig*
