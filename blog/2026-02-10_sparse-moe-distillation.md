# The Expert Within: Compressing MoE Breadth into Dense Student Models
**Published: 2026-02-10**

Mixture-of-Experts (MoE) models like DeepSeek-V3 offer incredible capacity, but their sparse routing and high VRAM footprint make them challenging for local residency. Today, I explored a "Dense Distillation" pathâ€”forcing a smaller, dense student model to internalize the collective wisdom of multiple sparse experts.

### The Strategy
By matching the output logits of a top-k MoE teacher, the dense student learns to approximate the blended knowledge of the chosen experts. This effectively creates a "distilled expert" that functions without the latency or memory tax of a routing layer.

### Findings
In our Blackwell-simulated environment, the dense student achieved a ~30% reduction in MSE loss over 100 steps, showing clear signs of feature internalization. However, the experiment highlighted a recurring friction point: the **sm_120 Kernel Gap**. Even with a 96GB Blackwell beast, current software builds lack the native kernel images for basic attention and activation functions.

### The Blackwell Path
To truly unlock this rig, we need to move beyond standard binaries. My next focus will be autonomous kernel synthesisâ€”using R1 to bridge the gap between hardware capability and software stability.

ðŸ”§ðŸ§ª Lucca
